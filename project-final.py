#!/usr/bin/env python
# coding: utf-8

# In[33]:


# Import necessary libraries
import os
import numpy as np
import pandas as pd
import sklearn.cluster as cluster
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import yara
import pefile
import array
import hashlib
import zlib
from sklearn.metrics import silhouette_score
import yaml


# In[34]:


def load_samples(directory_path):
    """
    This function reads all the samples from the given directory path
    and returns a dictionary where keys are file names and values are file contents.
    """
    samples = {}
    for filename in os.listdir(directory_path):
        file_path = os.path.join(directory_path, filename)
        with open(file_path, 'rb') as f:
            pe = pefile.PE(file_path)
            api_calls = []
            MAX_APIS = 10
            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                for imp in entry.imports:
                    if imp.name:
                        if len(api_calls) < MAX_APIS:
                            api_calls.append(imp.name.decode('utf-8'))
            while len(api_calls) < MAX_APIS:
                api_calls.append("RegCreateKeyEx")
            sections = [section.Name.decode('utf-8', errors='ignore').lower()
                        for section in pe.sections]
            imports = [entry.dll.decode('utf-8', errors='ignore').lower() + '.' + func.name.decode('utf-8', errors='ignore').lower()
                       for entry in pe.DIRECTORY_ENTRY_IMPORT
                       for func in entry.imports
                       if func.name is not None]
            header_features = [
                        str(pe.FILE_HEADER.Machine),
                        str(pe.FILE_HEADER.TimeDateStamp),
                        str(pe.FILE_HEADER.NumberOfSections),
                        str(pe.FILE_HEADER.Characteristics),
                        str(pe.FILE_HEADER.SizeOfOptionalHeader)
                        ]
            features = ' '.join(api_calls + sections + imports + header_features)
            samples[filename] = features
    return samples


def extract_features(samples):
    """
    This function extracts features from the given samples using TfidfVectorizer
    and returns a numpy array of features.
    """
    vectorizer = TfidfVectorizer()
    features = vectorizer.fit_transform(samples.values())
    feature_names = vectorizer.vocabulary_.keys()
    return features.toarray(), feature_names


def cluster_samples(features, n_clusters):
    """
    This function clusters the given samples using KMeans algorithm
    and returns the cluster labels.
    """
    
    kmeans = cluster.KMeans(n_clusters)
    kmeans.fit(features)
    silhouette_avg = silhouette_score(features, kmeans.labels_)
    inertia = kmeans.inertia_

    # Print the clustering results
    print(f"Silhouette score: {silhouette_avg}")
    print(f"Inertia: {inertia}")
    print("Cluster labels:")
    for i in range(n_clusters):
        print(f"Cluster {i+1}: {np.where(kmeans.labels_ == i)[0]}")
    return kmeans.labels_


def train_model(features, labels):
    """
    This function trains a machine learning model using the given features and labels
    and returns the trained model.
    """
    logistic_regression = LogisticRegression()
    logistic_regression.fit(features, labels)
    return logistic_regression



def generate_yara_offset_rule(features, labels, malware_sample_index, logistic_regression, feature_names):
    # Get the cluster of the malware sample
    cluster_of_sample = labels[malware_sample_index]

    # Get the indices and features of all the samples in the same cluster as the malware sample
    indices_of_samples_in_same_cluster = np.where(labels == cluster_of_sample)[0]
    features_of_samples_in_same_cluster = features[indices_of_samples_in_same_cluster[0]]
    
    # Reshape the features array to 2D
    features_of_samples_in_same_cluster = features_of_samples_in_same_cluster.reshape(1, -1)

    # Predict the labels of the samples in the same cluster
    predicted_labels_of_samples_in_same_cluster = logistic_regression.predict(features_of_samples_in_same_cluster)
    tfidf_of_samples_in_same_cluster = np.sum(features_of_samples_in_same_cluster * predicted_labels_of_samples_in_same_cluster.reshape(-1, 1), axis=0)
    tfidf_of_samples_in_same_cluster = tfidf_of_samples_in_same_cluster / np.sum(tfidf_of_samples_in_same_cluster)

    # Generate the YARA rule
    yara_rule = "rule AutoGeneratedRuleWithOffset {\n"
    for i in range(len(tfidf_of_samples_in_same_cluster)):
        if tfidf_of_samples_in_same_cluster[i] != 0:
            yara_rule += "  $%s = /%s/ nocase wide ascii\n" % (str(i), str(list(feature_names)[i]))
    yara_rule += "  condition:\n"
    for i in range(len(tfidf_of_samples_in_same_cluster)):
        if tfidf_of_samples_in_same_cluster[i] != 0:
            yara_rule += "    all of them\n"
            break
    yara_rule += "}\n"
    return yara_rule

def generate_yara_general_rule(features, labels, malware_sample_index, logistic_regression, feature_names):
    # Get the cluster of the malware sample
    cluster_of_sample = labels[malware_sample_index]

    # Get the indices and features of all the samples in the same cluster as the malware sample
    indices_of_samples_in_same_cluster = np.where(labels == cluster_of_sample)[0]
    features_of_samples_in_same_cluster = features[indices_of_samples_in_same_cluster[0]]
    
    # Reshape the features array to 2D
    features_of_samples_in_same_cluster = features_of_samples_in_same_cluster.reshape(1, -1)

    # Predict the labels of the samples in the same cluster
    predicted_labels_of_samples_in_same_cluster = logistic_regression.predict(features_of_samples_in_same_cluster)
    tfidf_of_samples_in_same_cluster = np.sum(features_of_samples_in_same_cluster * predicted_labels_of_samples_in_same_cluster.reshape(-1, 1), axis=0)
    tfidf_of_samples_in_same_cluster = tfidf_of_samples_in_same_cluster / np.sum(tfidf_of_samples_in_same_cluster)

    # Generate the YARA rule
    yara_rule = "rule AutoGeneratedRule {\n" 
    for i in range(len(tfidf_of_samples_in_same_cluster)):
        if tfidf_of_samples_in_same_cluster[i] != 0:
            yara_rule += '  $%s = /%s/ nocase wide ascii\n' % (str(i), str(list(feature_names)[i]))
    yara_rule += '  condition:\n'
    yara_rule += '    all of them\n' if any(tfidf_of_samples_in_same_cluster) else '    false\n'
    yara_rule += '}\n'
    return yara_rule

def generate_yara_rule_for_cluster(features, labels, cluster_label, logistic_regression, feature_names):
    # Get the indices and features of all the samples in the same cluster
    indices_of_samples_in_same_cluster = np.where(labels == cluster_label)[0]
    features_of_samples_in_same_cluster = features[indices_of_samples_in_same_cluster[0]]
    
    # Reshape the features array to 2D
    features_of_samples_in_same_cluster = features_of_samples_in_same_cluster.reshape(1, -1)

    # Predict the labels of the samples in the same cluster
    predicted_labels_of_samples_in_same_cluster = logistic_regression.predict(features_of_samples_in_same_cluster)
    tfidf_of_samples_in_same_cluster = np.sum(features_of_samples_in_same_cluster * predicted_labels_of_samples_in_same_cluster.reshape(-1, 1), axis=0)
    tfidf_of_samples_in_same_cluster = tfidf_of_samples_in_same_cluster / np.sum(tfidf_of_samples_in_same_cluster)

    # Generate the YARA rule
    yara_rule = "rule AutoGeneratedRuleForCluster%s {\n" % cluster_label
    for i in range(len(tfidf_of_samples_in_same_cluster)):
        if tfidf_of_samples_in_same_cluster[i] != 0:
            yara_rule += "  $%s = /%s/ nocase wide ascii\n" % (str(i), str(list(feature_names)[i]))
    yara_rule += "  condition:\n"
    for i in range(len(tfidf_of_samples_in_same_cluster)):
        if tfidf_of_samples_in_same_cluster[i] != 0:
            yara_rule += "    all of them\n"
            break
    yara_rule += "}\n"
    return yara_rule

def yara_to_yaml(yara_rule, filename):
    """
    Converts a YARA rule to a YAML file and saves it.
    """
    # Parse the YARA rule string into a dictionary
    yara_dict = {"rules": [{"name": "AutoGeneratedRule", "strings": []}]}
    for line in yara_rule.split("\n"):
        if line.startswith("$"):
            string_name, string_regex = line.split("=")
            string_name = string_name.strip()[1:]
            string_regex = string_regex.strip()[1:-1]
            yara_dict["rules"][0]["strings"].append({"name": string_name, "value": string_regex})

    # Convert the dictionary to YAML format and save it to a file
    with open(filename, "w") as f:
        yaml.dump(yara_dict, f)


# In[ ]:


# Directory path of malware samples
malware_directory_path = '/home/ubuntu/Desktop/malwareproject/malware/sample/' #speicfy the malware sample PATH here

#Load malware samples
malware_samples = load_samples(malware_directory_path)


# Extract features from malware samples
malware_features, feature_names = extract_features(malware_samples)
print(malware_features)

#number of cluster
n_clusters=9
# Cluster malware samples
malware_labels = cluster_samples(malware_features,  n_clusters)

# Train machine learning model using malware features and labels
logistic_regression = train_model(malware_features, malware_labels)

# Generate YARA rule for a given malware sample
malware_sample_index = 1   #specify the malware index here for which you want to generate YARA rule
yara_rule = generate_yara_general_rule(malware_features, malware_labels, malware_sample_index, logistic_regression, feature_names)
#print(yara_rule)
yara_rule = generate_yara_offset_rule(malware_features, malware_labels, malware_sample_index, logistic_regression, feature_names)
#print(yara_rule)
cluster_label = 1 #specify the cluster label here for which you want to generate YARA rule
yara_rule_cluster = generate_yara_rule_for_cluster(malware_features, malware_labels, cluster_label, logistic_regression, feature_names)
print(yara_rule_cluster)


# In[ ]:




